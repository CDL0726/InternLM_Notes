## **online Courese**  
# **第5节 LMDeploy 大模型量化部署实践**
#### Jan 24 2024    长琴 ｜ HuggingLLM 开源项目负责人

[课程视频](https://www.bilibili.com/video/BV1iW4y1A77P/?vd_source=427d5b3bd6552cd66c00e381e2aae338)

[课程文档](https://github.com/InternLM/tutorial/blob/main/lmdeploy/lmdeploy.md)


## LMDeploy 的量化和部署

<!-- START doctoc generated TOC please keep comment here to allow auto update -->
<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->

- [1 环境配置](#1-%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE)
- [2 服务部署](#2-%E6%9C%8D%E5%8A%A1%E9%83%A8%E7%BD%B2)
  - [2.1 模型转换](#21-%E6%A8%A1%E5%9E%8B%E8%BD%AC%E6%8D%A2)
    - [2.1.1 在线转换](#211-%E5%9C%A8%E7%BA%BF%E8%BD%AC%E6%8D%A2)
    - [2.1.2 离线转换](#212-%E7%A6%BB%E7%BA%BF%E8%BD%AC%E6%8D%A2)
  - [2.2  TurboMind 推理+命令行本地对话](#22--turbomind-%E6%8E%A8%E7%90%86%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%9C%AC%E5%9C%B0%E5%AF%B9%E8%AF%9D)
  - [2.3 TurboMind推理+API服务](#23-turbomind%E6%8E%A8%E7%90%86api%E6%9C%8D%E5%8A%A1)
  - [2.4 网页 Demo 演示](#24-%E7%BD%91%E9%A1%B5-demo-%E6%BC%94%E7%A4%BA)
    - [2.4.1 TurboMind 服务作为后端](#241-turbomind-%E6%9C%8D%E5%8A%A1%E4%BD%9C%E4%B8%BA%E5%90%8E%E7%AB%AF)
    - [2.4.2 TurboMind 推理作为后端](#242-turbomind-%E6%8E%A8%E7%90%86%E4%BD%9C%E4%B8%BA%E5%90%8E%E7%AB%AF)
  - [2.5 TurboMind 推理 + Python 代码集成](#25-turbomind-%E6%8E%A8%E7%90%86--python-%E4%BB%A3%E7%A0%81%E9%9B%86%E6%88%90)
  - [2.6 这么多，头秃，有没有最佳实践](#26-%E8%BF%99%E4%B9%88%E5%A4%9A%E5%A4%B4%E7%A7%83%E6%9C%89%E6%B2%A1%E6%9C%89%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5)
    - [2.6.1 方案实践](#261-%E6%96%B9%E6%A1%88%E5%AE%9E%E8%B7%B5)
    - [2.6.2 模型配置实践](#262-%E6%A8%A1%E5%9E%8B%E9%85%8D%E7%BD%AE%E5%AE%9E%E8%B7%B5)
- [3 模型量化](#3-%E6%A8%A1%E5%9E%8B%E9%87%8F%E5%8C%96)
  - [3.1 KV Cache 量化](#31-kv-cache-%E9%87%8F%E5%8C%96)
    - [3.1.1 量化步骤](#311-%E9%87%8F%E5%8C%96%E6%AD%A5%E9%AA%A4)
    - [3.1.2 量化效果](#312-%E9%87%8F%E5%8C%96%E6%95%88%E6%9E%9C)
  - [3.2 W4A16 量化](#32-w4a16-%E9%87%8F%E5%8C%96)
    - [3.2.1 量化步骤](#321-%E9%87%8F%E5%8C%96%E6%AD%A5%E9%AA%A4)
    - [3.2.2 量化效果](#322-%E9%87%8F%E5%8C%96%E6%95%88%E6%9E%9C)
  - [3.3 最佳实践](#33-%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5)
- [参考资料](#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99)
- [附录1：TritonServer 作为推理引擎](#%E9%99%84%E5%BD%951tritonserver-%E4%BD%9C%E4%B8%BA%E6%8E%A8%E7%90%86%E5%BC%95%E6%93%8E)
  - [TritonServer环境配置](#tritonserver%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE)
  - [TritonServer推理+API服务](#tritonserver%E6%8E%A8%E7%90%86api%E6%9C%8D%E5%8A%A1)
  - [TritonServer 服务作为后端](#tritonserver-%E6%9C%8D%E5%8A%A1%E4%BD%9C%E4%B8%BA%E5%90%8E%E7%AB%AF)

## 安装、部署、量化 实践操作Tips
#### Code server 操作tips
    - exit() 程序退出命令
    - cd ~ 回到根目录

#### Conda 环境创建
    - 打开开发机，进入Code Serve界面
    - bash 
    - vgpu-smi 查看显卡资源使用情况
    - watch vgpu-smi 观察 GPU 资源的使用情况
    - /root/share/install_conda_env_internlm_base.sh lmdeploy 安装新的软件包
    - conda env list 本地查看环境
    - conda activate lmdeploy 激活conda环境

#### 服务部署
  - 模型转换
      - 在线转换, lmdeploy 支持直接读取 Huggingface 模型权重,
          - 直接启动本地的 Huggingface 模型
          - lmdeploy chat turbomind /share/temp/model_repos/internlm-chat-7b/  --model-name internlm-chat-7b
  -  离线转换
       - 将模型转为 lmdeploy TurboMind 的格式 
          - 使用官方提供的模型文件，就在用户根目录执行
          - lmdeploy convert internlm-chat-7b  /root/share/temp/model_repos/internlm-chat-7b/ 

  - TurboMind 推理+命令行本地对话
          - lmdeploy chat turbomind ./workspace 

  - TurboMind推理+API服务  
         - ”模型推理/服务“目前提供了 Turbomind 和 TritonServer 两种服务化方式。此时，Server 是 TurboMind 或 TritonServer，API Server 可以提供对外的 API 服务。  
           - 通过下面命令启动服务
            - lmdeploy serve api_server ./workspace \
	                --server_name 0.0.0.0 \
	                --server_port 23333 \
	                --instance_num 64 \
	                --tp 1 
             - 新开一个窗口，打开 vscode 的 Terminal，执行下面的命令
                  - lmdeploy serve api_client http://localhost:23333 


