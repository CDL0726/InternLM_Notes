# **第4节 XTuner 大模型单卡低成本微调实战**
#### Jan 18 2024    汪周谦 XTuner 社区贡献者
[课件文档](https://github.com/InternLM/tutorial/blob/main/xtuner/README.md)

## Finetune简介
#### 指令跟随微调  
#####   三个角色：system user assistant
#### 增量预训练微调

## XTuner 介绍
#### 0基础的非专业的微调模型工具
#### XTuner微调原理：LoRA & QLoRA
#### 多数据样本拼接(Pack Dataset)  自定义数据集格式Json 或 Jsonl  

## 8G显卡玩转LLM
### 二种优化技巧
####    Flashion Attention, Flash Attention 将 Attention 计算并行化避免了计算过程中 Attention Score NxN的显存占用 (训练过程中的 N 都比较大)
####    DeepSpeed ZeRO, ZeRO 优化，通过将训练过程中的参数、梯度和优化器状态切片保存，能够在多 GPU 训练时显著节省显存
除了将训练中间状态切片外，DeepSpeed 训练时使用 FP16 的权重，相较于 Pytorch 的AMP 训练在单 GPU上也能大幅节省显存   

## 实战
#### Windows PowerShell 链接SSH
   - Windnows 找开 Windows PowerShell 
   - ssh-keygen 输入 回车，直接出现 SHA256，说明公钥文件已经创建好了
   - cat .\.ssh\id_rsa.pub 公钥文件
#### 常用命令
   - Ctrl + C 中断微调
   - ls 查看文件 （openassistant-guanace, work_dirs）
   - rm -rf work_dirs  删除文件 work_dirs
   - apt update -y 对apt语言进行更新，在root文件下
   
#### 利用Tmux持续微调（关电脑情况下, 课件视频0:43:00 ~ 0：46:45/1:34:01）
   - Ctrl + C 中断微调
   - ls 查看文件 （openassistant-guanace, work_dirs）
   - rm -rf work_dirs  删除文件 work_dirs
   - apt update -y 对apt语言进行更新，在root文件下
   - apt install tmux -y 
   - tmux new -s finetune  创建tmux session 
   - 退出Finetune命令： Ctrl+B,松开后再 按 D
   - 回到Finetune命令： tmux attach -t finetune

#### XTuner下载时出错的解决方案：
###### 拉取 0.1.9 的版本源码
         - git clone -b v0.1.9  https://github.com/InternLM/xtuner
###### 无法访问github的用户请从 gitee 拉取:
         - git clone -b v0.1.9 https://gitee.com/Internlm/xtuner

[Serper API](https://serper.dev/api-key)


